{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d4595e-3acd-440f-8995-92807f8c8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Core Libraries for Data Handling and Visualization ---\n",
    "import pandas as pd                   # For data manipulation and analysis\n",
    "import numpy as np                    # For numerical operations and working with arrays\n",
    "import matplotlib.pyplot as plt       # For static data visualizations\n",
    "import seaborn as sns                 # For enhanced data visualization with statistical plots\n",
    "\n",
    "# --- Scikit-learn Modules for Preprocessing and Model Evaluation ---\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV   # Data splitting and hyperparameter tuning              \n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler     # For encoding categorical features and feature scaling\n",
    "from sklearn.pipeline import Pipeline                                # To streamline preprocessing and modeling steps\n",
    "from sklearn.compose import ColumnTransformer                        # To apply different preprocessing pipelines to different column types\n",
    "\n",
    "# --- Regression Models ---\n",
    "from sklearn.ensemble import RandomForestRegressor                 # Ensemble-based regression model\n",
    "from sklearn.linear_model import LinearRegression                  # Linear regression model for baseline comparison\n",
    "from xgboost import XGBRegressor                                   # Gradient boosting model known for high performance\n",
    "\n",
    "# --- Model Evaluation Metrics ---\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # To evaluate regression model performance\n",
    "\n",
    "# --- Utility for Saving Models ---\n",
    "import joblib                                  # For saving and loading trained machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6480a004-9e1a-4700-b2a3-3f314708933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Importing SSI Datasets for 2021, 2022, and 2023 ---\n",
    "\n",
    "# File paths to each year's Surgical Site Infection dataset\n",
    "ssi_2021 = r\"C:\\Users\\AKIN-JOHNSON\\Desktop\\Workspace\\TDI\\Stage 2\\ca_ssi_adult_odp_2021.csv\"\n",
    "ssi_2022 = r\"C:\\Users\\AKIN-JOHNSON\\Desktop\\Workspace\\TDI\\Stage 2\\ca_ssi_adult_odp_2022.csv\"\n",
    "ssi_2023 = r\"C:\\Users\\AKIN-JOHNSON\\Desktop\\Workspace\\TDI\\Stage 2\\ca_ssi_adult_odp_2023.csv\"\n",
    "\n",
    "# Reading the CSV files into separate DataFrames\n",
    "df1 = pd.read_csv(ssi_2021)  # 2021 dataset\n",
    "df2 = pd.read_csv(ssi_2022)  # 2022 dataset\n",
    "df3 = pd.read_csv(ssi_2023)  # 2023 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d26edc0-38ac-4d5f-889e-54e73a363b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 Data Shape: (6139, 18)\n",
      "2022 Data Shape: (6092, 18)\n",
      "2023 Data Shape: (6038, 18)\n"
     ]
    }
   ],
   "source": [
    "# --- Checking Dataset Sizes (Row and Column Count) ---\n",
    "\n",
    "# Print the shape (rows, columns) of the 2021 dataset\n",
    "print(\"2021 Data Shape:\", df1.shape)\n",
    "# Print the shape of the 2022 dataset\n",
    "print(\"2022 Data Shape:\", df2.shape)\n",
    "# Print the shape of the 2023 dataset\n",
    "print(\"2023 Data Shape:\", df3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf61e38-02b8-4d70-a104-77448e999526",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a2ff27-376d-438a-817f-32c1e284b2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# --- Verifying Column Consistency Across All Datasets ---\n",
    "\n",
    "# Store all yearly DataFrames in a list for easy iteration\n",
    "dataframes = [df1, df2, df3]\n",
    "# Extract column names from each dataset\n",
    "columns = [df.columns.to_list() for df in dataframes]\n",
    "# Display the list of columns for each year to check for alignment\n",
    "columns\n",
    "\n",
    "print(all(columns[0] == col for col in columns[1:]))  # Returns True if all match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c64ff4d-1076-41b4-a812-3dd057553ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>HAI</th>\n",
       "      <th>Operative_Procedure</th>\n",
       "      <th>Facility_ID</th>\n",
       "      <th>Facility_Name</th>\n",
       "      <th>Hospital_Category_RiskAdjustment</th>\n",
       "      <th>Facility_Type</th>\n",
       "      <th>Procedure_Count</th>\n",
       "      <th>Infections_Reported</th>\n",
       "      <th>Infections_Predicted</th>\n",
       "      <th>SIR</th>\n",
       "      <th>SIR_CI_95_Lower_Limit</th>\n",
       "      <th>SIR_CI_95_Upper_Limit</th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Met_2020_Goal</th>\n",
       "      <th>SIR_2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Appendix surgery</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Bile duct, liver or pancreatic surgery</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Cesarean section</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.82</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Colon surgery</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Exploratory abdominal surgery (laparotomy)</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.82</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year       State      County                             HAI  \\\n",
       "0  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "1  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "2  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "3  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "4  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "\n",
       "                          Operative_Procedure  Facility_ID  \\\n",
       "0                            Appendix surgery     30000037   \n",
       "1      Bile duct, liver or pancreatic surgery     30000037   \n",
       "2                            Cesarean section     30000037   \n",
       "3                               Colon surgery     30000037   \n",
       "4  Exploratory abdominal surgery (laparotomy)     30000037   \n",
       "\n",
       "                      Facility_Name Hospital_Category_RiskAdjustment  \\\n",
       "0  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "1  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "2  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "3  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "4  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "\n",
       "             Facility_Type  Procedure_Count  Infections_Reported  \\\n",
       "0  Community, 125-250 Beds               82                    0   \n",
       "1  Community, 125-250 Beds                6                    0   \n",
       "2  Community, 125-250 Beds              256                    0   \n",
       "3  Community, 125-250 Beds               36                    0   \n",
       "4  Community, 125-250 Beds               99                    0   \n",
       "\n",
       "   Infections_Predicted  SIR  SIR_CI_95_Lower_Limit  SIR_CI_95_Upper_Limit  \\\n",
       "0                  0.16  NaN                    NaN                    NaN   \n",
       "1                  0.05  NaN                    NaN                    NaN   \n",
       "2                  0.25  0.0                    0.0                  14.82   \n",
       "3                  0.70  0.0                    0.0                   5.25   \n",
       "4                  0.31  0.0                    0.0                  11.82   \n",
       "\n",
       "  Comparison Met_2020_Goal  SIR_2015  \n",
       "0        NaN           NaN      0.00  \n",
       "1        NaN           NaN       NaN  \n",
       "2       Same           Yes      0.00  \n",
       "3       Same           Yes      1.11  \n",
       "4       Same           Yes      0.00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Combine Datasets and Standardize Column Names ---\n",
    "\n",
    "# Concatenate all three years of data into a single DataFrame\n",
    "df = pd.concat([df1, df2, df3], axis=0).reset_index(drop=True)\n",
    "# Clean column names: remove leading/trailing spaces and replace spaces with underscores for consistency\n",
    "df.columns = df.columns.str.strip().str.replace(\" \", \"_\")\n",
    "# Display the combined DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d9f058-6999-4436-b0c5-d638c8759620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "SIR_2015                 9245\n",
      "Met_2020_Goal            8998\n",
      "Comparison               8998\n",
      "SIR_CI_95_Upper_Limit    8998\n",
      "SIR_CI_95_Lower_Limit    8998\n",
      "SIR                      8998\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Check for Missing Values ---\n",
    "\n",
    "# Display the total number of missing values in each column\n",
    "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Print columns with missing values (only if they exist)\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f45e13-5b9e-43af-afdb-20b9e0e16a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Check for Duplicate Rows ---\n",
    "\n",
    "# Count and print the number of duplicated rows in the dataset\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73670945-9639-4583-b92e-4b454cace08b",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd290d61-b2e5-459e-95f0-a49bb963f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after filtering 'SIR':\n",
      " index                                  0\n",
      "Year                                   0\n",
      "State                                  0\n",
      "County                                 0\n",
      "HAI                                    0\n",
      "Operative_Procedure                    0\n",
      "Facility_ID                            0\n",
      "Facility_Name                          0\n",
      "Hospital_Category_RiskAdjustment       0\n",
      "Facility_Type                          0\n",
      "Procedure_Count                        0\n",
      "Infections_Reported                    0\n",
      "Infections_Predicted                   0\n",
      "SIR                                    0\n",
      "SIR_CI_95_Lower_Limit                  0\n",
      "SIR_CI_95_Upper_Limit                  0\n",
      "Comparison                             0\n",
      "Met_2020_Goal                          0\n",
      "SIR_2015                            1717\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Handle Missing Values in 'SIR' Column ---\n",
    "\n",
    "# Remove rows where 'SIR' (Standardized Infection Ratio) is missing\n",
    "df = df[df[\"SIR\"].notnull()].reset_index()\n",
    "# Re-check for any remaining missing values in the dataset\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values after filtering 'SIR':\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d005ee6f-fede-4890-be45-f27d034aba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning numeric columns:\n",
      " index                               0\n",
      "Year                                0\n",
      "State                               0\n",
      "County                              0\n",
      "HAI                                 0\n",
      "Operative_Procedure                 0\n",
      "Facility_ID                         0\n",
      "Facility_Name                       0\n",
      "Hospital_Category_RiskAdjustment    0\n",
      "Facility_Type                       0\n",
      "Procedure_Count                     0\n",
      "Infections_Reported                 0\n",
      "Infections_Predicted                0\n",
      "SIR                                 0\n",
      "SIR_CI_95_Lower_Limit               0\n",
      "SIR_CI_95_Upper_Limit               0\n",
      "Comparison                          0\n",
      "Met_2020_Goal                       0\n",
      "SIR_2015                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Handle Missing or Invalid Entries in Numeric Columns ---\n",
    "\n",
    "# Define numeric columns that should only contain valid numerical values\n",
    "numeric_cols = [\n",
    "    \"Procedure_Count\", \n",
    "    \"Infections_Reported\", \n",
    "    \"Infections_Predicted\",\n",
    "    \"SIR\", \n",
    "    \"SIR_CI_95_Lower_Limit\", \n",
    "    \"SIR_CI_95_Upper_Limit\", \n",
    "    \"SIR_2015\"\n",
    "]\n",
    "\n",
    "# Replace blank strings and spaces with NaN, then convert to float\n",
    "df[numeric_cols] = df[numeric_cols].replace([\"\", \" \"], np.nan).astype(float)\n",
    "\n",
    "# Fill all remaining NaN values in numeric columns with 0\n",
    "# Note: This assumes missing = 0. Confirm this with domain logic or documentation.\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "# Check for any remaining missing values\n",
    "missing_values_post_cleaning = df.isnull().sum()\n",
    "print(\"Missing values after cleaning numeric columns:\\n\", missing_values_post_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe816f-701e-43de-92cd-9b987dcd90a8",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16de347f-1759-4a66-992b-e992fdbb9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Year</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>HAI</th>\n",
       "      <th>Operative_Procedure</th>\n",
       "      <th>Facility_ID</th>\n",
       "      <th>Facility_Name</th>\n",
       "      <th>Hospital_Category_RiskAdjustment</th>\n",
       "      <th>Facility_Type</th>\n",
       "      <th>Procedure_Count</th>\n",
       "      <th>Infections_Reported</th>\n",
       "      <th>Infections_Predicted</th>\n",
       "      <th>SIR</th>\n",
       "      <th>SIR_CI_95_Lower_Limit</th>\n",
       "      <th>SIR_CI_95_Upper_Limit</th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Met_2020_Goal</th>\n",
       "      <th>SIR_2015</th>\n",
       "      <th>Infection_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>California</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Surgical Site Infections (SSI)</td>\n",
       "      <td>Cesarean section</td>\n",
       "      <td>30000037</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.82</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Year       State      County                             HAI  \\\n",
       "0      2  2021  California  Sacramento  Surgical Site Infections (SSI)   \n",
       "\n",
       "  Operative_Procedure  Facility_ID                     Facility_Name  \\\n",
       "0    Cesarean section     30000037  Methodist Hospital of Sacramento   \n",
       "\n",
       "  Hospital_Category_RiskAdjustment            Facility_Type  Procedure_Count  \\\n",
       "0              Acute Care Hospital  Community, 125-250 Beds            256.0   \n",
       "\n",
       "   Infections_Reported  Infections_Predicted  SIR  SIR_CI_95_Lower_Limit  \\\n",
       "0                  0.0                  0.25  0.0                    0.0   \n",
       "\n",
       "   SIR_CI_95_Upper_Limit Comparison Met_2020_Goal  SIR_2015  Infection_Rate  \n",
       "0                  14.82       Same           Yes       0.0             0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Feature Engineering: Infection Rate ---\n",
    "# Create a new column 'Infection_Rate' as a proxy for infection risk\n",
    "# Adding +1 to the denominator to avoid division by zero (if any procedure count is 0)\n",
    "\n",
    "df[\"Infection_Rate\"] = df[\"Infections_Reported\"] / (df[\"Procedure_Count\"] + 1)\n",
    "# Preview the first row to confirm the new feature\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "405326ab-b224-4388-9de1-21e79b956114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Year</th>\n",
       "      <th>County</th>\n",
       "      <th>Operative_Procedure</th>\n",
       "      <th>Facility_Name</th>\n",
       "      <th>Hospital_Category_RiskAdjustment</th>\n",
       "      <th>Facility_Type</th>\n",
       "      <th>Procedure_Count</th>\n",
       "      <th>Infections_Reported</th>\n",
       "      <th>Infections_Predicted</th>\n",
       "      <th>SIR</th>\n",
       "      <th>SIR_CI_95_Lower_Limit</th>\n",
       "      <th>SIR_CI_95_Upper_Limit</th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Met_2020_Goal</th>\n",
       "      <th>SIR_2015</th>\n",
       "      <th>Infection_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Cesarean section</td>\n",
       "      <td>Methodist Hospital of Sacramento</td>\n",
       "      <td>Acute Care Hospital</td>\n",
       "      <td>Community, 125-250 Beds</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.82</td>\n",
       "      <td>Same</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Year      County Operative_Procedure  \\\n",
       "0      2  2021  Sacramento    Cesarean section   \n",
       "\n",
       "                      Facility_Name Hospital_Category_RiskAdjustment  \\\n",
       "0  Methodist Hospital of Sacramento              Acute Care Hospital   \n",
       "\n",
       "             Facility_Type  Procedure_Count  Infections_Reported  \\\n",
       "0  Community, 125-250 Beds            256.0                  0.0   \n",
       "\n",
       "   Infections_Predicted  SIR  SIR_CI_95_Lower_Limit  SIR_CI_95_Upper_Limit  \\\n",
       "0                  0.25  0.0                    0.0                  14.82   \n",
       "\n",
       "  Comparison Met_2020_Goal  SIR_2015  Infection_Rate  \n",
       "0       Same           Yes       0.0             0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Drop Redundant Columns ---\n",
    "# 'HAI': All rows refer to Surgical Site Infections (SSI), so it contains no variance.\n",
    "# 'Facility_ID': Already have 'Facility_Name', which is more interpretable for stakeholders.\n",
    "# 'State': All data are from California — it's a constant, so not useful for analysis or modeling.\n",
    "\n",
    "df = df.drop(columns=[\"HAI\", \"Facility_ID\", \"State\"])\n",
    "# Preview the cleaned dataset\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6e402-2a41-4f1d-b7ad-2f4d9135f023",
   "metadata": {},
   "source": [
    "### Setup for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0c45bdb-2f0d-49cd-acac-f747a27f0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature and Target Definition ---\n",
    "# 'SIR' (Standardized Infection Ratio) is the main metric used by health authorities\n",
    "# to evaluate surgical site infection performance. We'll predict this.\n",
    "\n",
    "X = df.drop(columns=[\"SIR\"])  # Features: all columns except the target\n",
    "y = df[\"SIR\"]                 # Target: Standardized Infection Ratio (SIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cefccfbb-4ddf-4f7c-9b07-53d2149033e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Separate Column Types for Preprocessing ---\n",
    "\n",
    "# List of categorical features based on domain understanding\n",
    "# These features represent labels or categories rather than continuous values\n",
    "categorical_cols = [\n",
    "    \"Operative_Procedure\", \n",
    "    \"Hospital_Category_RiskAdjustment\", \n",
    "    \"Facility_Type\",\n",
    "    \"Comparison\", \n",
    "    \"Met_2020_Goal\", \n",
    "    \"County\", \n",
    "    \"Facility_Name\"\n",
    "]\n",
    "\n",
    "# All other columns are assumed to be numerical\n",
    "# This ensures proper preprocessing (e.g., scaling or imputation) for numeric data\n",
    "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27834186-198d-4280-8713-2a1d2c8f9547",
   "metadata": {},
   "source": [
    "### Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63a1ef0c-fd99-453e-970b-a53faf19decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing Transformers ---\n",
    "\n",
    "# Ordinal Encoding for Categorical Features\n",
    "# Converts categorical variables into integer labels.\n",
    "# The 'handle_unknown' parameter ensures that any unseen category in test data is handled gracefully.\n",
    "categorical_transformer = OrdinalEncoder(\n",
    "    handle_unknown='use_encoded_value', \n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Standard Scaling for Numerical Features\n",
    "# Standardizes numeric columns to have mean = 0 and std = 1, improving model convergence.\n",
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "433163ef-99c0-44d5-a7d6-bc8ba8b7c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine Preprocessing Steps into a ColumnTransformer ---\n",
    "\n",
    "# The ColumnTransformer applies:\n",
    "# - Standard scaling to all numerical columns\n",
    "# - Ordinal encoding to all categorical columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_cols),  # Numeric: scale features to standard normal distribution\n",
    "        (\"cat\", categorical_transformer, categorical_cols)  # Categorical: encode with integer labels\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846558b8-eed0-4c68-86dd-b15d4e2c93e6",
   "metadata": {},
   "source": [
    "### Build Final Pipeline (Model + Preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22ab26a2-9cf4-4905-83ca-275610ed8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Regressor Models to Compare Performance ---\n",
    "\n",
    "# Dictionary of regression models for evaluation\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),  # Baseline linear model\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),  # Ensemble method; handles non-linearity well\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42)  # Gradient boosting model; strong performance with tabular data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a25fa1ea-cce5-4927-8c77-e32a7e3318e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split Data into Training and Testing Sets ---\n",
    "\n",
    "# Reserve 20% of the data for testing to evaluate model performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,        # 80% training, 20% testing\n",
    "    random_state=42       # Ensures reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "947601b7-e1a1-4323-a693-ec08c71acdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LinearRegression Performance Metrics:\n",
      "  Train RMSE: 0.4841\n",
      "  Test RMSE:  0.4935\n",
      "  Train MSE:  0.2343\n",
      "  Test MSE:   0.2436\n",
      "  Train MAE:  0.3668\n",
      "  Test MAE:   0.3784\n",
      "  Train R²:   0.8732\n",
      "  Test R²:    0.8763\n",
      "\n",
      "RandomForest Performance Metrics:\n",
      "  Train RMSE: 0.0335\n",
      "  Test RMSE:  0.0785\n",
      "  Train MSE:  0.0011\n",
      "  Test MSE:   0.0062\n",
      "  Train MAE:  0.0029\n",
      "  Test MAE:   0.0077\n",
      "  Train R²:   0.9994\n",
      "  Test R²:    0.9969\n",
      "\n",
      "XGBoost Performance Metrics:\n",
      "  Train RMSE: 0.0069\n",
      "  Test RMSE:  0.0459\n",
      "  Train MSE:  0.0000\n",
      "  Test MSE:   0.0021\n",
      "  Train MAE:  0.0037\n",
      "  Test MAE:   0.0129\n",
      "  Train R²:   1.0000\n",
      "  Test R²:    0.9989\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Multiple Regression Models Using a Consistent Pipeline ---\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline combining preprocessing and the current model\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", model)\n",
    "    ])\n",
    "    \n",
    "    # Fit model on training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_train_pred = pipe.predict(X_train)\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Display performance metrics\n",
    "    print(f\"\\n{name} Performance Metrics:\")\n",
    "    print(f\"  Train RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "    print(f\"  Test RMSE:  {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "    \n",
    "    print(f\"  Train MSE:  {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"  Test MSE:   {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "    \n",
    "    print(f\"  Train MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"  Test MAE:   {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "    \n",
    "    print(f\"  Train R²:   {r2_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"  Test R²:    {r2_score(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71270c-846e-4617-8286-144c273635d2",
   "metadata": {},
   "source": [
    "##### Insights: \n",
    "- Linear Regression: No sign of overfitting, Reliable but not the most accurate amongst the 3 models. A solid benchmark, but not optimal.\n",
    "- Random Forest: The model is slightly overfitting, but still generalizing well.\n",
    "- XGBoost: Severe case of overfitting but model still proves to generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baccd43b-9f66-488c-9019-2ce7802a8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost Performance:\n",
      "  Test RMSE: 0.0594900448464715\n",
      "  Train RMSE: 0.067263086282421\n",
      "  Train MSE: 0.004524322776236413\n",
      "  Test MSE: 0.0035390654358351905\n",
      "  Train MAE: 0.026683443678277706\n",
      "  Test MAE: 0.029009294063726442\n",
      "  Train R²: 0.9975513969432331\n",
      "  Test R²: 0.9982026090789091\n"
     ]
    }
   ],
   "source": [
    "# --- Define a Tuned XGBoost Regressor with Regularization to Reduce Overfitting ---\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,         # Number of boosting rounds (trees)\n",
    "    max_depth=4,              # Maximum depth of each tree\n",
    "    learning_rate=0.05,       # Step size shrinkage to prevent overfitting\n",
    "    subsample=0.8,            # Row sampling ratio for each boosting round\n",
    "    colsample_bytree=0.8,     # Feature sampling ratio per tree\n",
    "    min_child_weight=3,       # Minimum sum of instance weight needed in a child\n",
    "    gamma=0.1,                # Minimum loss reduction to make a further partition\n",
    "    reg_alpha=0.5,            # L1 regularization (encourages sparsity)\n",
    "    reg_lambda=1,             # L2 regularization (controls model complexity)\n",
    "    random_state=42           # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# --- Build a Pipeline: Preprocessing + XGBoost Regressor ---\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),   # Handles encoding and scaling\n",
    "    (\"regressor\", xgb_model)          # Trained regression model\n",
    "])\n",
    "\n",
    "# --- Train the Model ---\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# --- Generate Predictions ---\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# --- Evaluate and Print Model Performance ---\n",
    "print(\"Tuned XGBoost Performance:\")\n",
    "print(\"  Test RMSE:\",  np.sqrt(mean_squared_error(y_test, y_test_pred)))   # Root Mean Squared Error on test set\n",
    "print(\"  Train RMSE:\", np.sqrt(mean_squared_error(y_train, y_train_pred))) # Root Mean Squared Error on training set\n",
    "\n",
    "print(\"  Train MSE:\", mean_squared_error(y_train, y_train_pred))           # Mean Squared Error (train)\n",
    "print(\"  Test MSE:\", mean_squared_error(y_test, y_test_pred))             # Mean Squared Error (test)\n",
    "\n",
    "print(\"  Train MAE:\", mean_absolute_error(y_train, y_train_pred))         # Mean Absolute Error (train)\n",
    "print(\"  Test MAE:\", mean_absolute_error(y_test, y_test_pred))            # Mean Absolute Error (test)\n",
    "\n",
    "print(\"  Train R²:\", r2_score(y_train, y_train_pred))                     # R² Score (train)\n",
    "print(\"  Test R²:\", r2_score(y_test, y_test_pred))                        # R² Score (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6f200f-7736-414c-b796-310b252becc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "Best Parameters:\n",
      " {'regressor__colsample_bytree': 1.0, 'regressor__learning_rate': 0.1, 'regressor__max_depth': 5, 'regressor__n_estimators': 200, 'regressor__reg_alpha': 0, 'regressor__reg_lambda': 1, 'regressor__subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Define the XGBoost Pipeline with Preprocessing ---\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),                          # Step 1: Column-wise encoding & scaling\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', # Step 2: XGBoost model for regression\n",
    "                               random_state=42))             # Fixed seed for reproducibility\n",
    "])\n",
    "\n",
    "# --- Define Hyperparameter Grid for Tuning ---\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],         # Number of boosting rounds\n",
    "    'regressor__max_depth': [3, 5, 7],             # Maximum tree depth\n",
    "    'regressor__learning_rate': [0.01, 0.1],       # Shrinkage rate\n",
    "    'regressor__subsample': [0.8, 1.0],            # Row sampling ratio\n",
    "    'regressor__colsample_bytree': [0.8, 1.0],     # Feature sampling ratio per tree\n",
    "    'regressor__reg_alpha': [0, 0.1],              # L1 regularization (sparsity)\n",
    "    'regressor__reg_lambda': [1, 10]               # L2 regularization (ridge)\n",
    "}\n",
    "\n",
    "# --- Perform Grid Search with Cross-Validation ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_pipeline, \n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',   # Minimizing MSE (negated because scikit-learn maximizes by default)\n",
    "    cv=3,                               # 3-fold cross-validation\n",
    "    verbose=2,                          # Verbosity level to track progress\n",
    "    n_jobs=-1                           # Use all available processors\n",
    ")\n",
    "\n",
    "# --- Train Grid Search on Training Data ---\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Extract and Display the Best Pipeline and Parameters ---\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\\n\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bfec498-f030-41e4-a9fc-436d704e31c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Tuned XGBoost Performance:\n",
      "  ✅ Train RMSE: 0.0139\n",
      "  ✅ Test RMSE:  0.0451\n",
      "  ✅ Train MSE:  0.000194\n",
      "  ✅ Test MSE:   0.002036\n",
      "  ✅ Train MAE:  0.0069\n",
      "  ✅ Test MAE:   0.0127\n",
      "  ✅ Train R²:   0.999895\n",
      "  ✅ Test R²:    0.998966\n"
     ]
    }
   ],
   "source": [
    "# --- Make predictions using the best tuned model ---\n",
    "y_test_pred = best_xgb_model.predict(X_test)\n",
    "y_train_pred = best_xgb_model.predict(X_train)\n",
    "\n",
    "# --- Print Performance Metrics ---\n",
    "print(\"🔍 Tuned XGBoost Performance:\")\n",
    "print(f\"  ✅ Train RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}\")\n",
    "print(f\"  ✅ Test RMSE:  {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}\")\n",
    "print(f\"  ✅ Train MSE:  {mean_squared_error(y_train, y_train_pred):.6f}\")\n",
    "print(f\"  ✅ Test MSE:   {mean_squared_error(y_test, y_test_pred):.6f}\")\n",
    "print(f\"  ✅ Train MAE:  {mean_absolute_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"  ✅ Test MAE:   {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"  ✅ Train R²:   {r2_score(y_train, y_train_pred):.6f}\")\n",
    "print(f\"  ✅ Test R²:    {r2_score(y_test, y_test_pred):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb97a1-62e1-427b-afab-7b836ef0c51c",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e455198-c7c1-4d8f-ac5c-4f0c28523c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m top_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[0;32m      4\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(\n\u001b[1;32m----> 5\u001b[0m     data\u001b[38;5;241m=\u001b[39mfeature_importance_df\u001b[38;5;241m.\u001b[39mhead(top_n),\n\u001b[0;32m      6\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Important Features (Tuned XGBoost)\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature Importance\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot top 20 important features\n",
    "top_n = 20\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(\n",
    "    data=feature_importance_df.head(top_n),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(f\"Top {top_n} Important Features (Tuned XGBoost)\", fontsize=14)\n",
    "plt.xlabel(\"Feature Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature Name\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3251e4a6-d05a-4e17-ae57-107f4a2632ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Specifying the columns using strings is only supported for dataframes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:482\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 482\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m best_model_pipe \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_xgb_model)\n\u001b[0;32m      4\u001b[0m ])\n\u001b[1;32m----> 5\u001b[0m best_model_pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Extract preprocessed feature names\u001b[39;00m\n\u001b[0;32m      8\u001b[0m encoded_feature_names \u001b[38;5;241m=\u001b[39m numerical_cols \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(best_model_pipe\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m                                               \u001b[38;5;241m.\u001b[39mnamed_transformers_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcategories_[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, routed_params)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    409\u001b[0m     cloned_transformer,\n\u001b[0;32m    410\u001b[0m     X,\n\u001b[0;32m    411\u001b[0m     y,\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    414\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    415\u001b[0m     params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:906\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    904\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 906\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:496\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    495\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 496\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:484\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    482\u001b[0m     all_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m     )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    488\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [key]\n",
      "\u001b[1;31mValueError\u001b[0m: Specifying the columns using strings is only supported for dataframes."
     ]
    }
   ],
   "source": [
    "best_model_pipe = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", best_xgb_model)\n",
    "])\n",
    "best_model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Extract preprocessed feature names\n",
    "encoded_feature_names = numerical_cols + list(best_model_pipe.named_steps[\"preprocessor\"]\n",
    "                                              .named_transformers_[\"cat\"].categories_[0])\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_model_pipe.named_steps[\"regressor\"].feature_importances_\n",
    "feature_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": importances})\n",
    "\n",
    "# Top 15 features\n",
    "feature_df = feature_df.sort_values(by=\"Importance\", ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_df, x=\"Importance\", y=\"Feature\", palette=\"mako\")\n",
    "plt.title(\"Top 15 Important Features (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe236b-5146-44eb-8df2-a8be18062e78",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53ce7944-760c-4b9a-90f7-5b6a75274ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssi_unique_values.joblib']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model from grid search\n",
    "joblib.dump(best_xgb_model, 'ssi_xgb_model.joblib')\n",
    "\n",
    "# Save the preprocessor separately (in case you want to use it elsewhere)\n",
    "joblib.dump(preprocessor, 'ssi_preprocessor.joblib')\n",
    "\n",
    "# Save the column names for reference\n",
    "joblib.dump({\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'all_columns': X.columns.tolist()\n",
    "}, 'ssi_columns.joblib')\n",
    "\n",
    "# Save the unique values for categorical columns for dropdowns in Streamlit\n",
    "unique_values = {col: df[col].unique().tolist() for col in categorical_cols}\n",
    "joblib.dump(unique_values, 'ssi_unique_values.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9da1d-137c-4c67-b139-eb5447b5336e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
